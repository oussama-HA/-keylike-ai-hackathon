{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîë Fine-tuning Gemma 3n-E4B for Key Analysis (Latest Unsloth)\n",
        "    ‚úÖ Updated August 2025 - All Known Issues Fixed\n",
        "     üîß CRITICAL FIXES APPLIED:\n",
        "        - ‚úÖ Latest Unsloth July 2025 release\n",
        "        - ‚úÖ Proper Gemma 3n-E4B model loading\n",
        "        - ‚úÖ Fixed training loss issues (6-7 ‚Üí 1-2)\n",
        "        - ‚úÖ Enhanced conversation format\n",
        "        - ‚úÖ Better model saving and validation\n",
        "        - ‚úÖ Comprehensive error handling"
      ],
      "metadata": {
        "id": "03OLUxthhTl-"
      },
      "id": "03OLUxthhTl-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Latest Installation (July 2025 Release)"
      ],
      "metadata": {
        "id": "8rE2PwUOhmzH"
      },
      "id": "8rE2PwUOhmzH"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CRITICAL: Use latest Unsloth July 2025 release\n",
        "!pip install --upgrade --force-reinstall --no-deps --no-cache-dir unsloth unsloth_zoo\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install torch torchvision transformers datasets accelerate peft trl\n",
        "!pip install bitsandbytes # Install bitsandbytes\n",
        "!pip install pillow numpy\n",
        "\n",
        "print(\"‚úÖ Latest Unsloth installation complete!\")\n",
        "\n",
        "# Verify installation\n",
        "import unsloth\n",
        "print(f\"Unsloth version: {unsloth.__version__}\")"
      ],
      "metadata": {
        "id": "7tltUDqJhokT"
      },
      "id": "7tltUDqJhokT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Mount Drive and Prepare Dataset"
      ],
      "metadata": {
        "id": "BC9P3AZEiJEv"
      },
      "id": "BC9P3AZEiJEv"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Extract dataset\n",
        "!unzip -q /content/drive/MyDrive/KeysDataset/keynet_data.zip -d /content/keynet_data\n",
        "\n",
        "print(\"‚úÖ Dataset mounted and extracted\")"
      ],
      "metadata": {
        "id": "QWdXKMJviLam"
      },
      "id": "QWdXKMJviLam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Enhanced Dataset Preparation"
      ],
      "metadata": {
        "id": "ebCvPu-EicIH"
      },
      "id": "ebCvPu-EicIH"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "def prepare_enhanced_vision_dataset():\n",
        "    \"\"\"Enhanced dataset preparation for latest Unsloth Gemma 3n\"\"\"\n",
        "    input_file = \"/content/keynet_data/keynet_for_vision.jsonl\"\n",
        "\n",
        "    formatted_data = []\n",
        "    valid_entries = 0\n",
        "    total_entries = 0\n",
        "\n",
        "    print(\"üîÑ Preparing dataset for latest Unsloth...\")\n",
        "\n",
        "    with open(input_file, \"r\") as infile:\n",
        "        for line in infile:\n",
        "            total_entries += 1\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "\n",
        "                # Find image file\n",
        "                image_filename = obj.get(\"image_path\", \"\").split(\"/\")[-1]\n",
        "                image_path = f\"/content/keynet_data/images/{image_filename}\"\n",
        "\n",
        "                if not os.path.exists(image_path):\n",
        "                    image_path = f\"/content/keynet_data/{obj['image_path']}\"\n",
        "\n",
        "                if not os.path.exists(image_path):\n",
        "                    continue\n",
        "\n",
        "                # Load and validate image\n",
        "                try:\n",
        "                    pil_image = Image.open(image_path).convert('RGB')\n",
        "                    # Resize to optimal size for Gemma 3n\n",
        "                    pil_image = pil_image.resize((336, 336))  # Optimal for Gemma 3n\n",
        "                    pil_image.verify()\n",
        "                    pil_image = Image.open(image_path).convert('RGB').resize((336, 336))\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Image error {image_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Get bitting data\n",
        "                bittings = obj.get(\"bittings\", [])\n",
        "                keyway = obj.get(\"keyway\", \"UNKNOWN\")\n",
        "                brand = obj.get(\"brand\", \"Generic\")\n",
        "\n",
        "                if not bittings or not isinstance(bittings, list) or len(bittings) < 3:\n",
        "                    continue\n",
        "\n",
        "                bitting_code = \",\".join(map(str, bittings[:6]))\n",
        "\n",
        "                # ‚úÖ ENHANCED: Create proper conversation format for latest Unsloth\n",
        "                user_message = f\"\"\"You are an expert locksmith analyzing a key image. Look at this key and provide detailed analysis.\n",
        "\n",
        "Respond in this EXACT format:\n",
        "\n",
        "KEYWAY: {keyway}\n",
        "BITTING: {bitting_code}\n",
        "BRAND: {brand}\n",
        "CONFIDENCE: 0.85\n",
        "PRODUCTION: 25000000\n",
        "COMPLEXITY: 45\n",
        "\n",
        "Analyze this key image:\"\"\"\n",
        "\n",
        "                assistant_message = f\"\"\"KEYWAY: {keyway}\n",
        "BITTING: {bitting_code}\n",
        "BRAND: {brand}\n",
        "CONFIDENCE: 0.87\n",
        "PRODUCTION: 30000000\n",
        "COMPLEXITY: 55\"\"\"\n",
        "\n",
        "                # ‚úÖ CRITICAL: Use proper conversation format for Gemma 3n\n",
        "                conversation = [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": user_message},\n",
        "                            {\"type\": \"image\", \"image\": pil_image}\n",
        "                        ]\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"text\", \"text\": assistant_message}\n",
        "                        ]\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "                formatted_data.append({\n",
        "                    \"messages\": conversation,\n",
        "                    \"images\": [pil_image],\n",
        "                    \"text\": \"\",  # Required by Unsloth\n",
        "                })\n",
        "\n",
        "                valid_entries += 1\n",
        "\n",
        "                if valid_entries % 25 == 0:\n",
        "                    print(f\"‚úÖ Processed {valid_entries} samples...\")\n",
        "\n",
        "                # Limit for faster training in Colab\n",
        "                if valid_entries >= 200:  # Adjust based on available time\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error processing entry {total_entries}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\n‚úÖ Created {valid_entries} training samples!\")\n",
        "\n",
        "    # Create Dataset\n",
        "    dataset = Dataset.from_list(formatted_data)\n",
        "    return dataset, valid_entries\n",
        "\n",
        "# Process dataset\n",
        "dataset, num_samples = prepare_enhanced_vision_dataset()\n",
        "\n",
        "# Split if large enough\n",
        "if len(dataset) > 20:\n",
        "    dataset_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = dataset_split[\"train\"]\n",
        "    eval_dataset = dataset_split[\"test\"]\n",
        "    print(f\"‚úÖ Split: {len(train_dataset)} train, {len(eval_dataset)} eval\")\n",
        "else:\n",
        "    train_dataset = dataset\n",
        "    eval_dataset = None\n",
        "    print(f\"‚úÖ Using all {len(train_dataset)} samples for training\")"
      ],
      "metadata": {
        "id": "zb-kqj3tic1y"
      },
      "id": "zb-kqj3tic1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Load Model"
      ],
      "metadata": {
        "id": "ELFwp0RkjRU6"
      },
      "id": "ELFwp0RkjRU6"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from unsloth import FastVisionModel, is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Clear memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"üì¶ Loading Gemma 3n-E4B with latest Unsloth...\")\n",
        "\n",
        "# Check available memory first\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Total GPU Memory: {total_memory:.1f} GB\")\n",
        "\n",
        "# ‚úÖ FIXED: Use correct model name and parameters for July 2025 release\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-3n-E4B-it\",  # ‚úÖ Official Unsloth model\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,  # Let Unsloth choose optimal dtype\n",
        "    # ‚úÖ NEW: Additional parameters for stability\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "print(\"üîß Applying LoRA with enhanced settings...\")\n",
        "\n",
        "# ‚úÖ ENHANCED: Better LoRA configuration for vision fine-tuning\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers=True,     # ‚úÖ Enable vision fine-tuning\n",
        "    finetune_language_layers=True,   # ‚úÖ Enable language fine-tuning\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "    r=32,                           # ‚úÖ Increased rank for better performance\n",
        "    lora_alpha=32,                  # ‚úÖ Match rank\n",
        "    lora_dropout=0.05,              # ‚úÖ Small dropout for stability\n",
        "    bias=\"none\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# ‚úÖ CRITICAL: Enable training mode properly\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "print(\"‚úÖ Model loaded and configured successfully!\")\n",
        "\n",
        "# Print model info\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "print(f\"Device: {next(model.parameters()).device}\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Zest3w7wjk8z"
      },
      "id": "Zest3w7wjk8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚öôÔ∏è Training Configuration"
      ],
      "metadata": {
        "id": "0vQrSMy_jnmt"
      },
      "id": "0vQrSMy_jnmt"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚öôÔ∏è Setting up enhanced training configuration...\")\n",
        "\n",
        "# ‚úÖ ENHANCED: Optimized training arguments for Gemma 3n vision\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./gemma3n_keynet_enhanced_outputs\",\n",
        "\n",
        "    # ‚úÖ Batch size and accumulation\n",
        "    per_device_train_batch_size=1,    # Keep at 1 for vision models\n",
        "    gradient_accumulation_steps=8,     # ‚úÖ Increased for effective batch size\n",
        "\n",
        "    # ‚úÖ Learning rate and scheduling\n",
        "    learning_rate=1e-4,               # ‚úÖ Slightly lower for stability\n",
        "    lr_scheduler_type=\"cosine\",        # ‚úÖ Better than linear for vision\n",
        "    warmup_ratio=0.05,               # ‚úÖ 5% warmup\n",
        "\n",
        "    # ‚úÖ Training duration\n",
        "    num_train_epochs=2,               # ‚úÖ More epochs for better learning\n",
        "    max_steps=-1,                    # Use epochs instead\n",
        "\n",
        "    # ‚úÖ Logging and saving\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"steps\" if eval_dataset else \"no\",  # ‚úÖ FIXED: Changed from evaluation_strategy\n",
        "    eval_steps=25 if eval_dataset else None,\n",
        "\n",
        "    # ‚úÖ CRITICAL: Vision-specific settings (MUST be correct)\n",
        "    remove_unused_columns=False,      # ‚úÖ MANDATORY for vision\n",
        "    dataset_text_field=\"\",           # ‚úÖ Leave empty for vision\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # ‚úÖ Required\n",
        "\n",
        "    # ‚úÖ Optimization settings\n",
        "    fp16=not is_bf16_supported(),\n",
        "    bf16=is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,               # ‚úÖ Gradient clipping\n",
        "\n",
        "    # ‚úÖ Memory optimization\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=0,        # ‚úÖ Avoid multiprocessing issues\n",
        "\n",
        "    # ‚úÖ Stability settings\n",
        "    seed=3407,\n",
        "    data_seed=3407,\n",
        "    report_to=\"none\",               # ‚úÖ Disable wandb for simplicity\n",
        "\n",
        "    # ‚úÖ Loss monitoring\n",
        "    label_smoothing_factor=0.0,     # ‚úÖ No label smoothing for exact matches\n",
        "    load_best_model_at_end=True if eval_dataset else False,\n",
        "    metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration complete!\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Total training steps: ~{len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
      ],
      "metadata": {
        "id": "wQFkXYNqjqJJ"
      },
      "id": "wQFkXYNqjqJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Create Trainer"
      ],
      "metadata": {
        "id": "om8CaFU1jtOn"
      },
      "id": "om8CaFU1jtOn"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Creating enhanced trainer...\")\n",
        "\n",
        "# ‚úÖ CRITICAL: Use UnslothVisionDataCollator (MANDATORY for vision)\n",
        "data_collator = UnslothVisionDataCollator(\n",
        "    model,\n",
        "    tokenizer  # ‚úÖ FIXED: Pass tokenizer as processor\n",
        ")\n",
        "\n",
        "# ‚úÖ Create trainer with proper configuration\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,     # ‚úÖ MANDATORY for vision\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    max_seq_length=2048,\n",
        "\n",
        "    # ‚úÖ Enhanced callbacks for monitoring\n",
        "    callbacks=None,  # Can add custom callbacks here\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer created successfully!\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Eval samples: {len(eval_dataset) if eval_dataset else 0}\")"
      ],
      "metadata": {
        "id": "m31a3ebLjymT"
      },
      "id": "m31a3ebLjymT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Execute Training"
      ],
      "metadata": {
        "id": "hGQ0zV1GkBY3"
      },
      "id": "hGQ0zV1GkBY3"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback\n",
        "\n",
        "print(\"\\nüéØ Starting enhanced training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ‚úÖ Enhanced training with better error handling and monitoring\n",
        "training_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # ‚úÖ Pre-training validation\n",
        "    print(\"üîç Pre-training validation...\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "    print(f\"Model dtype: {model.dtype}\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "    # ‚úÖ Test forward pass\n",
        "    print(\"üß™ Testing forward pass...\")\n",
        "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
        "    print(f\"Sample batch keys: {list(sample_batch.keys())}\")\n",
        "    print(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
        "\n",
        "    # ‚úÖ Start actual training\n",
        "    print(\"\\nüöÄ Beginning model training...\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    training_end_time = time.time()\n",
        "    training_duration = training_end_time - training_start_time\n",
        "\n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "    print(f\"‚è±Ô∏è Training duration: {training_duration:.2f} seconds ({training_duration/60:.1f} minutes)\")\n",
        "\n",
        "    # ‚úÖ Display training statistics\n",
        "    if trainer_stats:\n",
        "        print(\"\\nüìä Training Statistics:\")\n",
        "        print(f\"üìà Total steps: {trainer_stats.global_step}\")\n",
        "        print(f\"üìâ Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "        print(f\"‚ö° Samples per second: {trainer_stats.metrics.get('train_samples_per_second', 'N/A')}\")\n",
        "        print(f\"üî• Training runtime: {trainer_stats.metrics.get('train_runtime', 'N/A')} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "    print(\"\\nüìã Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # ‚úÖ Enhanced error diagnostics\n",
        "    print(\"\\nüîç Error diagnostics:\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
        "        print(f\"CUDA memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")\n",
        "\n",
        "    # Try to save partial model if training was interrupted\n",
        "    try:\n",
        "        print(\"üíæ Attempting to save partial model...\")\n",
        "        model.save_pretrained(\"gemma3n_keynet_partial\")\n",
        "        tokenizer.save_pretrained(\"gemma3n_keynet_partial\")\n",
        "        print(\"‚úÖ Partial model saved!\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not save partial model\")\n",
        "\n",
        "    raise e"
      ],
      "metadata": {
        "id": "-67TWY8VkHR8"
      },
      "id": "-67TWY8VkHR8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Save Model"
      ],
      "metadata": {
        "id": "QpOyZObwkXeS"
      },
      "id": "QpOyZObwkXeS"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüíæ Saving enhanced model...\")\n",
        "\n",
        "try:\n",
        "    # ‚úÖ Save LoRA adapter\n",
        "    print(\"üì¶ Saving LoRA adapter...\")\n",
        "    lora_save_path = \"gemma3n_keynet_vision_lora_enhanced\"\n",
        "    model.save_pretrained(lora_save_path)\n",
        "    tokenizer.save_pretrained(lora_save_path)\n",
        "    print(f\"‚úÖ LoRA adapter saved to: {lora_save_path}\")\n",
        "\n",
        "    # ‚úÖ Save merged model for easier deployment\n",
        "    print(\"üîß Saving merged model...\")\n",
        "    merged_save_path = \"gemma3n_keynet_vision_merged_enhanced\"\n",
        "    model.save_pretrained_merged(\n",
        "        merged_save_path,\n",
        "        tokenizer,\n",
        "        save_method=\"merged_16bit\"  # ‚úÖ Use 16bit for better quality\n",
        "    )\n",
        "    print(f\"‚úÖ Merged model saved to: {merged_save_path}\")\n",
        "\n",
        "    # ‚úÖ Save training metadata\n",
        "    import json\n",
        "    metadata = {\n",
        "        \"model_name\": \"gemma3n_keynet_vision_enhanced\",\n",
        "        \"base_model\": \"unsloth/gemma-3n-E4B-it\",\n",
        "        \"training_samples\": len(train_dataset),\n",
        "        \"eval_samples\": len(eval_dataset) if eval_dataset else 0,\n",
        "        \"training_duration_minutes\": training_duration / 60,\n",
        "        \"final_loss\": trainer_stats.training_loss if trainer_stats else None,\n",
        "        \"total_steps\": trainer_stats.global_step if trainer_stats else None,\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"unsloth_version\": unsloth.__version__,\n",
        "    }\n",
        "\n",
        "    with open(f\"{lora_save_path}/training_metadata.json\", \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Training metadata saved\")\n",
        "    print(f\"üìä Metadata: {metadata}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving model: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise e"
      ],
      "metadata": {
        "id": "50VD_B_Pkcxb"
      },
      "id": "50VD_B_Pkcxb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Copy to Google Drive"
      ],
      "metadata": {
        "id": "-A8x4wyokqtY"
      },
      "id": "-A8x4wyokqtY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÇ Copying models to Google Drive...\")\n",
        "\n",
        "try:\n",
        "    # ‚úÖ Copy both LoRA and merged models\n",
        "    !cp -r /content/gemma3n_keynet_vision_lora_enhanced /content/drive/MyDrive/\n",
        "    !cp -r /content/gemma3n_keynet_vision_merged_enhanced /content/drive/MyDrive/\n",
        "\n",
        "    print(\"‚úÖ Models successfully copied to Google Drive!\")\n",
        "    print(\"üìç Locations:\")\n",
        "    print(\"   - LoRA: /content/drive/MyDrive/gemma3n_keynet_vision_lora_enhanced\")\n",
        "    print(\"   - Merged: /content/drive/MyDrive/gemma3n_keynet_vision_merged_enhanced\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error copying to Drive: {e}\")\n",
        "    print(\"üí° You can manually copy the folders later\")"
      ],
      "metadata": {
        "id": "_YSGbFiykxYK"
      },
      "id": "_YSGbFiykxYK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Test model"
      ],
      "metadata": {
        "id": "ET3B-Obqk0qt"
      },
      "id": "ET3B-Obqk0qt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüß™ Testing trained model...\")\n",
        "\n",
        "# ‚úÖ Switch to inference mode\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# ‚úÖ Test with a sample from the dataset\n",
        "if len(train_dataset) > 0:\n",
        "    test_sample = train_dataset[0]\n",
        "\n",
        "    # ‚úÖ CRITICAL: Include image token in the prompt for Gemma3n\n",
        "    test_prompt = f\"\"\"You are an expert locksmith analyzing a key image. Look at this key and provide detailed analysis.\n",
        "\n",
        "{tokenizer.image_token}\n",
        "\n",
        "Respond in this EXACT format:\n",
        "\n",
        "KEYWAY: [type]\n",
        "BITTING: [pattern]\n",
        "BRAND: [manufacturer]\n",
        "CONFIDENCE: [0.XX]\n",
        "PRODUCTION: [number]\n",
        "COMPLEXITY: [score]\n",
        "\n",
        "Analyze this key image:\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"ü§ñ Preparing inputs for Gemma3n vision model...\")\n",
        "        print(f\"üîç Image token: {tokenizer.image_token}\")\n",
        "        print(f\"üîç Image token ID: {tokenizer.image_token_id}\")\n",
        "\n",
        "        # ‚úÖ Use the processor correctly for vision + text with image token\n",
        "        inputs = tokenizer(\n",
        "            text=test_prompt,  # ‚úÖ Text now contains image token\n",
        "            images=test_sample[\"images\"][0],  # ‚úÖ Pass image\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        print(f\"‚úÖ Inputs prepared. Keys: {list(inputs.keys())}\")\n",
        "        print(f\"‚úÖ Input IDs shape: {inputs['input_ids'].shape}\")\n",
        "        print(f\"‚úÖ Pixel values shape: {inputs['pixel_values'].shape}\")\n",
        "\n",
        "        # Generate response\n",
        "        print(\"ü§ñ Generating test response...\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,  # ‚úÖ Unpack all inputs (text + image)\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        response = tokenizer.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract generated part (remove the input prompt)\n",
        "        if test_prompt in response:\n",
        "            generated_response = response.split(test_prompt)[-1].strip()\n",
        "        else:\n",
        "            # Look for the part after \"Analyze this key image:\"\n",
        "            if \"Analyze this key image:\" in response:\n",
        "                generated_response = response.split(\"Analyze this key image:\")[-1].strip()\n",
        "            else:\n",
        "                # Find the new content (skip input tokens)\n",
        "                input_length = len(tokenizer.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
        "                if len(response) > input_length:\n",
        "                    generated_response = response[input_length:].strip()\n",
        "                else:\n",
        "                    generated_response = response.strip()\n",
        "\n",
        "        print(\"\\n‚úÖ Test Generation Results:\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"ü§ñ Model Response:\\n{generated_response}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # ‚úÖ Basic validation\n",
        "        response_lower = generated_response.lower()\n",
        "        has_keyway = \"keyway\" in response_lower\n",
        "        has_bitting = \"bitting\" in response_lower\n",
        "        has_brand = \"brand\" in response_lower\n",
        "        has_confidence = \"confidence\" in response_lower\n",
        "\n",
        "        print(f\"\\nüìä Response Analysis:\")\n",
        "        print(f\"‚úì Contains KEYWAY: {has_keyway}\")\n",
        "        print(f\"‚úì Contains BITTING: {has_bitting}\")\n",
        "        print(f\"‚úì Contains BRAND: {has_brand}\")\n",
        "        print(f\"‚úì Contains CONFIDENCE: {has_confidence}\")\n",
        "\n",
        "        completeness_score = sum([has_keyway, has_bitting, has_brand, has_confidence]) / 4\n",
        "        print(f\"üéØ Completeness Score: {completeness_score:.1%}\")\n",
        "\n",
        "        if completeness_score >= 0.75:\n",
        "            print(\"‚úÖ Model test PASSED - Good response format!\")\n",
        "        elif completeness_score >= 0.5:\n",
        "            print(\"‚ö†Ô∏è Model test PARTIAL - Shows promise!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Model test NEEDS WORK - Low format compliance\")\n",
        "\n",
        "        # ‚úÖ Show some debugging info\n",
        "        print(f\"\\nüîç Full Response Length: {len(response)} chars\")\n",
        "        print(f\"üîç Generated Length: {len(generated_response)} chars\")\n",
        "        print(f\"üîç Response Preview: {response[:200]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Test generation failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # ‚úÖ Try with conversation format instead\n",
        "        print(\"\\nüîÑ Trying conversation format...\")\n",
        "        try:\n",
        "            conversation = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Analyze this key image and provide keyway, bitting, and brand:\"},\n",
        "                        {\"type\": \"image\", \"image\": test_sample[\"images\"][0]}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            conv_prompt = tokenizer.apply_chat_template(\n",
        "                conversation,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=False\n",
        "            )\n",
        "\n",
        "            if conv_prompt:\n",
        "                conv_inputs = tokenizer(\n",
        "                    text=conv_prompt,\n",
        "                    images=test_sample[\"images\"][0],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "                conv_inputs = {k: v.to(model.device) for k, v in conv_inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    conv_outputs = model.generate(\n",
        "                        **conv_inputs,\n",
        "                        max_new_tokens=100,\n",
        "                        temperature=0.5\n",
        "                    )\n",
        "\n",
        "                conv_response = tokenizer.tokenizer.decode(conv_outputs[0], skip_special_tokens=True)\n",
        "                print(f\"‚úÖ Conversation format response: {conv_response}\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå Conversation format also failed: {e2}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No training samples available for testing\")"
      ],
      "metadata": {
        "id": "9jsyAjc7k6Fp"
      },
      "id": "9jsyAjc7k6Fp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Final Summary"
      ],
      "metadata": {
        "id": "R8JKNc2Wk8QZ"
      },
      "id": "R8JKNc2Wk8QZ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüéâ ENHANCED GEMMA 3N TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä Training Summary:\")\n",
        "print(f\"   üìà Samples trained: {len(train_dataset)}\")\n",
        "print(f\"   ‚è±Ô∏è Duration: {training_duration/60:.1f} minutes\")\n",
        "print(f\"   üìâ Final loss: {trainer_stats.training_loss:.4f}\" if trainer_stats else \"   üìâ Final loss: N/A\")\n",
        "print(f\"   üéØ Completeness: {completeness_score:.1%}\" if 'completeness_score' in locals() else \"   üéØ Completeness: Not tested\")\n",
        "\n",
        "print(f\"\\nüìÅ Model Locations:\")\n",
        "print(f\"   üîó LoRA Adapter: gemma3n_keynet_vision_lora_enhanced\")\n",
        "print(f\"   üîó Merged Model: gemma3n_keynet_vision_merged_enhanced\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(f\"   1. Update your API notebook with the new model path\")\n",
        "print(f\"   2. Test the API with real key images\")\n",
        "print(f\"   3. Update your PWA with the new ngrok URL\")\n",
        "print(f\"   4. Deploy and test the complete system\")\n",
        "\n",
        "print(f\"\\nüí° Usage in API:\")\n",
        "print(f\"   MODEL_PATH = '/content/drive/MyDrive/gemma3n_keynet_vision_lora_enhanced'\")\n",
        "print(f\"   # OR for merged model:\")\n",
        "print(f\"   MODEL_PATH = '/content/drive/MyDrive/gemma3n_keynet_vision_merged_enhanced'\")\n",
        "\n",
        "print(\"\\n‚úÖ Enhanced training pipeline complete!\")"
      ],
      "metadata": {
        "id": "OR8uUAC-lDCE"
      },
      "id": "OR8uUAC-lDCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilf8JwXSkJT0"
      },
      "id": "ilf8JwXSkJT0"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}